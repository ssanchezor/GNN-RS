{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfJJMXbnd_j8"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_eY6XbvpFYO"
      },
      "outputs": [],
      "source": [
        " #%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5393eb29"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "logs_base_dir = \"runs2\"\n",
        "os.makedirs(logs_base_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bf5c56c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "tb_PopularityRS = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_PopularityRS/')\n",
        "tb_RandomRS = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_RandomRS/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4xTDuV9itpA"
      },
      "outputs": [],
      "source": [
        "Drive = 1\n",
        "Path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMYX9gcdsnLu",
        "outputId": "6c408c83-e321-4d6a-b457-a881b8b55618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/LabData\n"
          ]
        }
      ],
      "source": [
        "if Drive == 1:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  %cd /content/drive/My Drive/LabData/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dd7e9f7"
      },
      "outputs": [],
      "source": [
        "if Drive == 0: Path = \"./data/\" \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jguvh_1ga6-3"
      },
      "source": [
        "# Código Solución Python (Bernat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo-YPv9ySt0Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def add_label_column(transaction, value):\n",
        "    transaction[\"label\"] = value  # add label column with fixed value\n",
        "    return transaction\n",
        "\n",
        "\n",
        "def extract_dictionary(transaction):\n",
        "    cust_dict = {};\n",
        "    art_dict = {}\n",
        "    count_cust = 1;\n",
        "    count_art = 1;\n",
        "    for index, row in transaction.iterrows():\n",
        "        customer=row[\"customer_id\"]; article=row[\"article_id\"]\n",
        "        if (customer not in cust_dict):\n",
        "            cust_dict[customer]=count_cust; count_cust+=1\n",
        "        if (article not in art_dict):\n",
        "            art_dict[article]=count_art; count_art+=1\n",
        "    return cust_dict, art_dict\n",
        "\n",
        "\n",
        "def generate_datasets(transaction, cust_dict, art_dict):\n",
        "    test_data_list = [];\n",
        "    train_data_list = [];\n",
        "    last_customer_id = -999;\n",
        "    current_customer_id = -999\n",
        "    for index, row in transaction.iterrows():\n",
        "        customer = row[\"customer_id\"];\n",
        "        customer_id = cust_dict[customer]\n",
        "        article = row[\"article_id\"];\n",
        "        article_id = art_dict[article]\n",
        "        timestamp = int(row[\"t_dat\"].replace('-', ''))\n",
        "        if (last_customer_id != customer_id):\n",
        "            current_customer_id = customer_id\n",
        "            last_customer_id = customer_id\n",
        "            row = [current_customer_id, article_id, row[\"label\"], timestamp]\n",
        "            test_data_list.append(row)\n",
        "        else:\n",
        "             row = [current_customer_id, article_id, row[\"label\"], timestamp]\n",
        "             train_data_list.append(row)\n",
        "    len_test=len(test_data_list)\n",
        "    len_train=len(train_data_list)\n",
        "    if __name__ == \"build_dataset\":\n",
        "        print(f' \\tTest dataset generated, length:: {len_test}')\n",
        "        print(f' \\tTrain dataset generated, length: {len_train}')\n",
        "    return test_data_list, train_data_list\n",
        "\n",
        "\n",
        "def build_adj_mx(dims, interactions):\n",
        "    adj_mat = sp.dok_matrix((dims, dims), dtype=np.float32)\n",
        "    for x in tqdm(interactions, desc=\"BUILDING ADJACENCY MATRIX...\"):\n",
        "        adj_mat[x[0], x[1]] = 1.0\n",
        "        adj_mat[x[1], x[0]] = 1.0\n",
        "    return adj_mat\n",
        "\n",
        "\n",
        "class CustomerArticleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_path, num_negatives_train=4, num_negatives_test=100, sep='\\t'):\n",
        "        number_customers = 10000\n",
        "        column_names = [\"customer_id\", \"article_id\", \"label\", \"t_dat\"]\n",
        "        train_data = pd.read_csv(f'{dataset_path}customer.train.article', sep=sep,\n",
        "                                 header=None, names=column_names).to_numpy()\n",
        "        test_data = pd.read_csv(f'{dataset_path}customer.test.article', sep=sep,\n",
        "                                header=None, names=column_names).to_numpy()\n",
        "\n",
        "        # TAKE items, targets and test_items\n",
        "        self.targets = train_data[:, 2]\n",
        "        self.items = self.preprocess_items(train_data, number_customers)\n",
        "\n",
        "        # Save dimensions of max users and items and build training matrix\n",
        "        self.field_dims = np.max(self.items, axis=0) + 1  # ([ 943, 2625])\n",
        "        self.train_mat = build_adj_mx(self.field_dims[-1], self.items.copy())\n",
        "\n",
        "        # Generate train interactions with 4 negative samples for each positive\n",
        "        self.negative_sampling(num_negatives=num_negatives_train)\n",
        "\n",
        "        # Build test set by passing as input the test item interactions\n",
        "        self.test_set = self.build_test_set(self.preprocess_items(test_data, number_customers),\n",
        "                                            num_neg_samples_test=num_negatives_test)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.targets.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.interactions[index]\n",
        "\n",
        "    def preprocess_items(self, data, num_customers):\n",
        "        reindexed_items = data[:, :2].astype(np.int)\n",
        "        reindexed_items[:, 1] = reindexed_items[:, 1] + num_customers\n",
        "        return reindexed_items\n",
        "\n",
        "    def negative_sampling(self, num_negatives):\n",
        "        self.interactions = []\n",
        "        data = np.c_[(self.items, self.targets)].astype(int)\n",
        "        max_users, max_items = self.field_dims[:2]  # number users (943), number items (2625)\n",
        "\n",
        "        for x in tqdm(data, desc=\"Performing negative sampling on test data...\"):  # x are triplets (u, i , 1)\n",
        "            # Append positive interaction\n",
        "            self.interactions.append(x)\n",
        "            # Copy user and maintain last position to 0. Now we will need to update neg_triplet[1] with j\n",
        "            neg_triplet = np.vstack([x, ] * (num_negatives))\n",
        "            neg_triplet[:, 2] = np.zeros(num_negatives)\n",
        "\n",
        "            # Generate num_negatives negative interactions\n",
        "            for idx in range(num_negatives):\n",
        "                j = np.random.randint(max_users, max_items)\n",
        "                # IDEA: Loop to exclude true interactions (set to 1 in adj_train) user - item\n",
        "                while (x[0], j) in self.train_mat:\n",
        "                    j = np.random.randint(max_users, max_items)\n",
        "                neg_triplet[:, 1][idx] = j\n",
        "            self.interactions.append(neg_triplet.copy())\n",
        "\n",
        "        self.interactions = np.vstack(self.interactions)\n",
        "\n",
        "    def build_test_set(self, gt_test_interactions, num_neg_samples_test):\n",
        "        max_users, max_items = self.field_dims[:2]  # number users (943), number items (2625)\n",
        "        test_set = []\n",
        "        for pair in tqdm(gt_test_interactions, desc=\"BUILDING TEST SET...\"):\n",
        "            negatives = []\n",
        "            for t in range(num_neg_samples_test):\n",
        "                j = np.random.randint(max_users, max_items)\n",
        "                while (pair[0], j) in self.train_mat or j == pair[1]:\n",
        "                    j = np.random.randint(max_users, max_items)\n",
        "                negatives.append(j)\n",
        "            # APPEND TEST SETS FOR SINGLE USER\n",
        "            single_user_test_set = np.vstack([pair, ] * (len(negatives) + 1))\n",
        "            single_user_test_set[:, 1][1:] = negatives\n",
        "            test_set.append(single_user_test_set.copy())\n",
        "        return test_set\n",
        "\n",
        "\n",
        "transactions = pd.read_csv(Path + \"transactions_ddup_2019-09-22_nart_5_ncust_20_ncustr_10000.csv\")\n",
        "transactions = add_label_column(transactions, 1)\n",
        "transactions = transactions.sort_values(['customer_id', 't_dat'], ascending=[True, False])\n",
        "\n",
        "customer_dict, article_dict = extract_dictionary(transactions)\n",
        "test_dataset, train_dataset = generate_datasets(transactions, customer_dict, article_dict)\n",
        "\n",
        "column_names = [\"customer_id\", \"article_id\", \"label\", \"t_dat\"]\n",
        "train_data = pd.DataFrame(train_dataset, columns= column_names)\n",
        "test_data = pd.DataFrame(test_dataset, columns= column_names)\n",
        "train_data.to_csv(Path + \"customer.train.article\" , sep=\"\\t\", index=False,header=False)\n",
        "test_data.to_csv(Path + \"customer.test.article\" , sep=\"\\t\", index=False,header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFXFXeQ4Tq6T",
        "outputId": "cc539648-7d4c-4887-ec15-1b056ff7082b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:95: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "BUILDING ADJACENCY MATRIX...: 100%|██████████| 397946/397946 [00:10<00:00, 38019.13it/s]\n",
            "Performing negative sampling on test data...: 100%|██████████| 397946/397946 [00:13<00:00, 29055.63it/s]\n",
            "BUILDING TEST SET...: 100%|██████████| 10000/10000 [00:05<00:00, 1857.55it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"\"\n",
        "full_dataset = CustomerArticleDataset(dataset_path, num_negatives_train=4, num_negatives_test=99)\n",
        "#data_loader = DataLoader(full_dataset, batch_size=256, shuffle=True, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROXEAYUpd5vv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def getHitRatio(recommend_list, gt_item):\n",
        "    if gt_item in recommend_list:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def getNDCG(recommend_list, gt_item):\n",
        "    idx = np.where(recommend_list == gt_item)[0]\n",
        "    if len(idx) > 0:\n",
        "        return math.log(2)/math.log(idx+2)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def getCoverage(total_recommended_items, total_items):\n",
        "    return total_recommended_items/total_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmrIE3CHb1EF"
      },
      "source": [
        "# Popularity Recommender System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl0QRXxeJs4i"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas\n",
        "\n",
        "class Popularity_Recommender():\n",
        "\n",
        "\t# Initialize all the variables\n",
        "\tdef __init__(self):\n",
        "\t\t# Tha training data which is been provided.\n",
        "\t\tself.train_data = None #interactions\n",
        "\t\tself.user_id = None #Column for customers or users\n",
        "\t\tself.item_id = None #Column for articles or items\n",
        "\t\tself.popularity_recommendations = None #Final recommendation list\n",
        "\n",
        "\t# Create the recommendations.\n",
        "\tdef create(self,train_data,user_id,item_id,label):\n",
        "\n",
        "\t\tself.train_data = train_data\n",
        "\t\tself.user_id = user_id\n",
        "\t\tself.item_id = item_id\n",
        "\t\tself.label = label\n",
        "\t\t\n",
        "\t\t# The items are grouped by item_id aggregated with the sum of 1 in the labels, we only count the real interactions.\n",
        "\t\t#train_data_grouped = train_data.groupby([self.item_id]).agg({self.user_id: 'count'}).reset_index()\n",
        "\t\ttrain_data_grouped = train_data.groupby([self.item_id]).agg({self.label: 'sum'}).reset_index() \n",
        "\t\ttrain_data_grouped.rename(columns = {self.label : 'score'}, inplace = True)\n",
        "\t\ttrain_data_sort = train_data_grouped.sort_values(['score', self.item_id], ascending = [0,1])\n",
        "\t\t# The new column named Rank is created by score sorted in ascending order.\n",
        "\t\ttrain_data_sort['Rank'] = train_data_sort['score'].rank(ascending = 0, method = 'first')\n",
        "\n",
        "\t\tself.popularity_recommendations = train_data_sort\n",
        "\n",
        "\n",
        "\t# Method to user created recommendations\n",
        "\tdef predict(self, user_id, topk=10):\n",
        "\n",
        "\t\tuser_recommendation = self.popularity_recommendations\n",
        "    #Delete the items that have been bought by the user before\n",
        "\t\tprevious_items_customer = self.train_data[self.train_data[self.user_id].eq(user_id) & self.train_data[self.label]>0 ][self.item_id]\n",
        "\t\tuser_recommendation = self.popularity_recommendations[~self.popularity_recommendations[self.item_id].isin(previous_items_customer)]\n",
        "\n",
        "\t\treturn user_recommendation.head(topk)[self.item_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAUW4m-7GCj2",
        "outputId": "4df8f6b3-cddc-4491-d2c6-98f036f45f37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[    1, 10002,     1],\n",
              "       [    1, 40132,     0],\n",
              "       [    1, 13990,     0],\n",
              "       ...,\n",
              "       [10000, 20683,     0],\n",
              "       [10000, 31647,     0],\n",
              "       [10000, 24842,     0]])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_dataset.interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIVjYxsHK6sB"
      },
      "outputs": [],
      "source": [
        "model = Popularity_Recommender()\n",
        "model.create(pd.DataFrame(full_dataset.interactions, columns=['customer_id','article_id','label']), 'customer_id', 'article_id','label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9VO0eAHK9C9",
        "outputId": "f70ac757-eaae-43b6-d9a6-33e2e5bef394"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16      10017\n",
              "1414    11415\n",
              "1514    11515\n",
              "145     10146\n",
              "1736    11737\n",
              "Name: article_id, dtype: int64"
            ]
          },
          "execution_count": 194,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(full_dataset.items[4000][0],5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8j76ejUK87z"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "def test_popularity(model, full_dataset, topk=10):\n",
        "    # Test the HR and NDCG for the model @topK\n",
        "    HR, NDCG, COVERAGE = [], [], []\n",
        "\n",
        "    for user_test in full_dataset.test_set:\n",
        "        gt_item = user_test[0][1]\n",
        "        predictions = model.predict(user_test[0][0], topk) #device\n",
        "        recommend_list = predictions\n",
        "        for art in recommend_list:  COVERAGE.append(art) if art not in COVERAGE else COVERAGE\n",
        "        HR.append(getHitRatio(recommend_list, gt_item))\n",
        "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
        "    return mean(HR), mean(NDCG), COVERAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SYER1RIe9Sf",
        "outputId": "81ceaf63-3d43-4385-ba55-3c9aee3feed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topk: 10\n",
            "initial HR:  0\n",
            "initial NDCG:  0.0036927681667798477\n",
            "initial COVERAGE:  0.00041546571109553115\n",
            "Topk: 50\n",
            "initial HR:  0\n",
            "initial NDCG:  0.007284893343239425\n",
            "initial COVERAGE:  0.001635896237438654\n",
            "Topk: 100\n",
            "initial HR:  0\n",
            "initial NDCG:  0.009758870636682165\n",
            "initial COVERAGE:  0.003012126405442601\n",
            "Topk: 300\n",
            "initial HR:  0.0001\n",
            "initial NDCG:  0.015249354332938591\n",
            "initial COVERAGE:  0.008543013684401858\n",
            "Topk: 500\n",
            "initial HR:  0.0003\n",
            "initial NDCG:  0.018351483101688038\n",
            "initial COVERAGE:  0.013918101321700294\n",
            "Topk: 1000\n",
            "initial HR:  0.0026\n",
            "initial NDCG:  0.023686418136420195\n",
            "initial COVERAGE:  0.027420736932305057\n"
          ]
        }
      ],
      "source": [
        "# Check Init performance\n",
        "for i in [10, 50, 100, 300, 500, 1000]:\n",
        "    hr, ndcg, cov = test_popularity(model, full_dataset, topk=i)\n",
        "\n",
        "    print(\"Topk:\", i)\n",
        "    print(\"initial HR: \", hr)\n",
        "    print(\"initial NDCG: \", ndcg)  \n",
        "\n",
        "    Coverage = getCoverage(len(cov), len(np.unique(full_dataset.interactions[:,1],)))\n",
        "    print(\"initial COVERAGE: \", Coverage)\n",
        "    \n",
        "    tb_PopularityRS.add_scalar('eval/COVERAGE@{topk}', Coverage, i)\n",
        "    tb_PopularityRS.add_scalar('eval/HR@{topk}', hr, i)\n",
        "    tb_PopularityRS.add_scalar('eval/NDCG@{topk}', ndcg, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vPU_Ijjq4RJ"
      },
      "source": [
        "# Random Recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EjNX-LZrIGA"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import random\n",
        "\n",
        "class Random_Recommender():\n",
        "\n",
        "\t# Initialize all the variables\n",
        "\tdef __init__(self):\n",
        "\t\t# Tha training data which is been provided.\n",
        "\t\tself.train_data = None #interactions\n",
        "\t\tself.user_id = None #Column for customers or users\n",
        "\t\tself.item_id = None #Column for articles or items\n",
        "\t\tself.list_items = None\n",
        "\t\tself.random_recommendations = None #Final recommendation list\n",
        "\n",
        "\t# Create the recommendations.\n",
        "\tdef create(self,train_data,user_id,item_id,label):\n",
        "\n",
        "\t\tself.train_data = train_data\n",
        "\t\tself.user_id = user_id\n",
        "\t\tself.item_id = item_id\n",
        "\t\t\n",
        "\t\tlist_items = list(train_data[self.item_id].unique())\n",
        "\t\tself.list_items = list_items\n",
        "\n",
        "\t# Method to user created recommendations\n",
        "\tdef predict(self, user_id, topk=10):\n",
        "\t\n",
        "    #Delete the items that have been bought by the user before\n",
        "\t\t#previous_items_customer = self.train_data[self.train_data[self.user_id].eq(user_id) & self.train_data[self.label]>0 ][self.item_id]\n",
        "\t\t#user_recommendation = self.random_recommendations[~self.random_recommendations.isin(previous_items_customer)]\n",
        "\t\t#user_recommendation = [rec.append(x) for x in user_recommendation if x not in previous_items_customer ]\n",
        "\t\t#print(user_recommendation[:topk])\n",
        "\t\tself.random_recommendations = self.list_items\n",
        "\t\trandom.shuffle(self.random_recommendations)\n",
        "\t\treturn self.random_recommendations[:topk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVdNaVQBtVdg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4M22ZZNtVrV"
      },
      "outputs": [],
      "source": [
        "model_random = Random_Recommender()\n",
        "model_random.create(pd.DataFrame(full_dataset.interactions, columns=['customer_id','article_id','label']), 'customer_id', 'article_id','label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slzcPKGCtVrW",
        "outputId": "0c944f0f-811a-4c26-e4bd-9ec861e728b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[39003, 33913, 28677, 21316, 18629]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_random.predict(full_dataset.items[100][0],5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzY9xi_4tVrW"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "def test_random(model, full_dataset, topk=10):\n",
        "    # Test the HR and NDCG for the model @topK\n",
        "    HR, NDCG, COVERAGE = [], [], []\n",
        "\n",
        "    for user_test in full_dataset.test_set:\n",
        "        gt_item = user_test[0][1]\n",
        "        predictions = model.predict(user_test[0][0], topk) #device\n",
        "        recommend_list = predictions\n",
        "        for art in recommend_list:  COVERAGE.append(art) if art not in COVERAGE else COVERAGE\n",
        "        HR.append(getHitRatio(recommend_list, gt_item))\n",
        "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
        "    return mean(HR), mean(NDCG), COVERAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nIJJIy8tsAz",
        "outputId": "952422d5-aa1c-41bb-92fc-fefe7572acc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topk: 10\n",
            "initial HR:  0.0004\n",
            "initial NDCG:  0.00013883320077684737\n",
            "initial COVERAGE:  0.9263067694944301\n",
            "Topk: 50\n",
            "initial HR:  0.0015\n",
            "initial NDCG:  0.00037428144152842755\n",
            "initial COVERAGE:  1.0\n",
            "Topk: 100\n",
            "initial HR:  0.0026\n",
            "initial NDCG:  0.0005103057800542293\n",
            "initial COVERAGE:  1.0\n",
            "Topk: 300\n",
            "initial HR:  0.0072\n",
            "initial NDCG:  0.0011343281135425872\n",
            "initial COVERAGE:  1.0\n",
            "Topk: 500\n",
            "initial HR:  0.0128\n",
            "initial NDCG:  0.0017788138294628495\n",
            "initial COVERAGE:  1.0\n",
            "Topk: 1000\n",
            "initial HR:  0.0245\n",
            "initial NDCG:  0.0031863902593946706\n",
            "initial COVERAGE:  1.0\n"
          ]
        }
      ],
      "source": [
        "# Check Init performance\n",
        "for i in [10, 50, 100, 300, 500, 1000]:\n",
        "    hr, ndcg, cov = test_random(model_random, full_dataset, topk=i)\n",
        "\n",
        "    print(\"Topk:\", i)\n",
        "    print(\"initial HR: \", hr)\n",
        "    print(\"initial NDCG: \", ndcg)  \n",
        "\n",
        "    Coverage = getCoverage(len(cov), len(np.unique(full_dataset.interactions[:,1],)))\n",
        "    print(\"initial COVERAGE: \", Coverage)\n",
        "    \n",
        "    tb_RandomRS.add_scalar('eval/COVERAGE@{topk}', Coverage, i)\n",
        "    tb_RandomRS.add_scalar('eval/HR@{topk}', hr, i)\n",
        "    tb_RandomRS.add_scalar('eval/NDCG@{topk}', ndcg, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WiDHv86q8-q"
      },
      "source": [
        "# VISUALIZING RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqNqBmKBvd9Y",
        "outputId": "ee0f4ee7-d232-4238-ac40-04ca76a0800a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Known TensorBoard instances:\n",
            "  - port 6006: logdir runs (started 0:02:33 ago; pid 2117)\n"
          ]
        }
      ],
      "source": [
        "#from tensorboard import notebook\n",
        "#notebook.list() # View open TensorBoard instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbSDTbdNdH3o",
        "outputId": "01d15119-4730-44cd-a767-a3bae49647f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ]
        }
      ],
      "source": [
        "%tensorboard --logdir runs2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Popularity&RandomRS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}