{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0059c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9a2e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"You should enable GPU runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ca1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca187a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "!pip install -q torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca0534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking problematic imports\n",
    "from torch_geometric.nn import GCNConv, GATConv  \n",
    "from torch_geometric.utils import from_scipy_sparse_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b1106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ab6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f6a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    " #%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5393eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "logs_base_dir = \"runs2\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf5c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "tb_fm = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_FM/')\n",
    "tb_gcn = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_GCN/')\n",
    "tb_gcn_attention = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_GCN_att/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2b3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1386016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9d3cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython import embed\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch_geometric.nn import GCNConv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76cf87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fa673c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions = pd.read_csv(\"./data/transactions_ddup_2019-09-22_nart_5_ncust_20_ncustr_25000.csv\")\n",
    "#transactions = pd.read_csv(\"./data/transactions_ddup_2019-09-22_nart_5_ncust_20_ncustr_15000.csv\")\n",
    "transactions = pd.read_csv(\"./data/transactions_ddup_2019-09-22_nart_5_ncust_20_ncustr_10000.csv\")\n",
    "number_cust_file=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "529e879a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16856320</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...</td>\n",
       "      <td>733098009</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16856321</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...</td>\n",
       "      <td>337991001</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16856322</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...</td>\n",
       "      <td>752814002</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16856323</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...</td>\n",
       "      <td>808938001</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16856324</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...</td>\n",
       "      <td>778187002</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       t_dat                                        customer_id  \\\n",
       "0    16856320  2019-09-22  032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...   \n",
       "1    16856321  2019-09-22  032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...   \n",
       "2    16856322  2019-09-22  032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...   \n",
       "3    16856323  2019-09-22  032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...   \n",
       "4    16856324  2019-09-22  032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...   \n",
       "\n",
       "   article_id     price  sales_channel_id  \n",
       "0   733098009  0.016932                 2  \n",
       "1   337991001  0.025407                 2  \n",
       "2   752814002  0.033881                 2  \n",
       "3   808938001  0.050831                 2  \n",
       "4   778187002  0.067780                 2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3feb485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions2=transactions[[\"customer_id\",\"article_id\", \"t_dat\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb24741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label wih 1\n",
    "transactions[\"label\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0e448e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          407717\n",
       "t_dat                  367\n",
       "customer_id          10000\n",
       "article_id           38349\n",
       "price                 4693\n",
       "sales_channel_id         2\n",
       "label                    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e6515cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16856320</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...</td>\n",
       "      <td>733098009</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16856321</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...</td>\n",
       "      <td>337991001</td>\n",
       "      <td>0.025407</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16856322</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...</td>\n",
       "      <td>752814002</td>\n",
       "      <td>0.033881</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       t_dat                                        customer_id  \\\n",
       "0    16856320  2019-09-22  032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...   \n",
       "1    16856321  2019-09-22  032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...   \n",
       "2    16856322  2019-09-22  032d6afde041a1f88cf96e7680e40bb7b03c425b7e409f...   \n",
       "\n",
       "   article_id     price  sales_channel_id  label  \n",
       "0   733098009  0.016932                 2      1  \n",
       "1   337991001  0.025407                 2      1  \n",
       "2   752814002  0.033881                 2      1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transactions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e268d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to remove test data\n",
    "# Test data es for each customer its hast transaction\n",
    "transactions=transactions.sort_values(['customer_id','t_dat'], \\\n",
    "              ascending = [True, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c1588d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create 2 dictionaries to save customer ID and article ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9180405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a36043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique customers ->  10000\n",
      "Number of unique articles  ->  38349\n"
     ]
    }
   ],
   "source": [
    "customer_dict={}; article_dict={}\n",
    "n_cust=1; n_art=1; debugi=0\n",
    "for index, row in transactions.iterrows():\n",
    "    customer=row[\"customer_id\"]; article=row[\"article_id\"]\n",
    "    if (customer not in customer_dict):\n",
    "        customer_dict[customer]=n_cust; n_cust+=1\n",
    "    if (article not in article_dict):\n",
    "        article_dict[article]=n_art; n_art+=1\n",
    "    #debugi+=1\n",
    "    #if (debugi==2000):\n",
    "    #    break;\n",
    "print (\"Number of unique customers -> \", len(customer_dict))\n",
    "print (\"Number of unique articles  -> \", len(article_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22f958",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72e8158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_debug=0;\n",
    "test_data_list=[]; data_list=[]; \n",
    "last_customer_id=-999; current_customer_id=-999\n",
    "for index, row in transactions.iterrows():\n",
    "    customer=row[\"customer_id\"]; customer_id= customer_dict[customer]\n",
    "    article= row[\"article_id\"]; article_id= article_dict[article]\n",
    "    timestamp = int (row[\"t_dat\"].replace('-',''))\n",
    "    if (last_customer_id != customer_id):\n",
    "        current_customer_id=customer_id # for data_test_list\n",
    "        last_customer_id= customer_id\n",
    "        row= [current_customer_id, article_id, row[\"label\"],timestamp ]\n",
    "        test_data_list.append(row)\n",
    "        aux_debug+=1\n",
    "        if (aux_debug == 70000):\n",
    "            print (\"<ERROR>\")\n",
    "            break\n",
    "    else:\n",
    "        row= [current_customer_id, article_id, row[\"label\"],timestamp ]\n",
    "        data_list.append(row)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d01c998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print (len (test_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdfb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00e32c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes to save as movielens.train.rating\n",
    "column_names = [\"customer_id\", \"article_id\" ,\"label\", \"t_dat\"]\n",
    "aux_data= pd.DataFrame(data_list, columns = column_names)\n",
    "aux_test_data=pd.DataFrame(test_data_list, columns = column_names)\n",
    "aux_data.to_csv(\"./data/movielens.train.rating\" , sep=\"\\t\", index=False,header=False) \n",
    "aux_test_data.to_csv(\"./data/movielens.test.rating\" , sep=\"\\t\", index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0056e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       1,        2,        1, 20200519],\n",
       "       [       1,        3,        1, 20200504],\n",
       "       [       1,        4,        1, 20200504],\n",
       "       ...,\n",
       "       [   10000,     7760,        1, 20191220],\n",
       "       [   10000,    14435,        1, 20191220],\n",
       "       [   10000,    24119,        1, 20191220]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing library\n",
    "import numpy\n",
    "# customer_id, artilce_id, label, t_dat\n",
    "\n",
    "data=numpy.array(data_list)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df840ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105365/273686876.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  items = data[:, :2].astype(np.int) - 1  # -1 because ID begins from 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     1],\n",
       "       [    0,     2],\n",
       "       [    0,     3],\n",
       "       ...,\n",
       "       [ 9999,  7759],\n",
       "       [ 9999, 14434],\n",
       "       [ 9999, 24118]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# items is cero based no need to dcrecese 1\n",
    "items = data[:, :2].astype(np.int) - 1  # -1 because ID begins from 1\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "803996b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10000, 38349])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I assume articles ID are fine, no need to add nothing but I will follow Paula flow\n",
    "np.max(items, axis=0)[:2] + 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ecf6404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 10001],\n",
       "       [    0, 10002],\n",
       "       [    0, 10003],\n",
       "       ...,\n",
       "       [ 9999, 17759],\n",
       "       [ 9999, 24434],\n",
       "       [ 9999, 34118]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need each node to have a unique id\n",
    "#\n",
    "#\n",
    "# notice 25000 comes from previous data\n",
    "# number_cust_file number of cust in file\n",
    "#\n",
    "#\n",
    "reindex_items = items.copy()\n",
    "reindex_items[:, 1] = reindex_items[:, 1] + number_cust_file\n",
    "reindex_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6624421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10000, 48349])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not sure what does\n",
    "field_dims = np.max(reindex_items, axis=0) + 1\n",
    "field_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dca72de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adj_mx(dims, interactions):\n",
    "    train_mat = sp.dok_matrix((dims, dims), dtype=np.float32)\n",
    "    for x in tqdm(interactions, desc=\"BUILDING ADJACENCY MATRIX...\"):\n",
    "        train_mat[x[0], x[1]] = 1.0\n",
    "        train_mat[x[1], x[0]] = 1.0\n",
    "\n",
    "    return train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2a47a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BUILDING ADJACENCY MATRIX...: 100%|█████████████████████████████████████████████████████████████████| 397717/397717 [00:05<00:00, 73114.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<48349x48349 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 749344 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mat = build_adj_mx(field_dims[-1], reindex_items.copy())\n",
    "train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7a09f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198114"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# this is for paulas, dont need it here \n",
    "# Check that we have (2*99057 = 198114) interactions...\n",
    "99057*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be1a3c",
   "metadata": {},
   "source": [
    "##### *Checking we have just positive data:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c913fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = data[:, 2]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7f966c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534ac5e",
   "metadata": {},
   "source": [
    "##### *Example on performing negative data for a training sample: (u, i, j)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a01fe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 10001,     1],\n",
       "       [    0, 10002,     1],\n",
       "       [    0, 10003,     1],\n",
       "       ...,\n",
       "       [ 9999, 17759,     1],\n",
       "       [ 9999, 24434,     1],\n",
       "       [ 9999, 34118,     1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.c_[(reindex_items, targets)].astype(int)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9a6fe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10000, 48349])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_dims[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d64d5bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   20, 10898,     1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXAMPLE interaction number 988 : user 6 - item 1470\n",
    "x = data[988]\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfeea5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20,  0,  0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_triplet = np.array([0,0,0])\n",
    "neg_triplet[0] = x[0].copy()\n",
    "neg_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff8949f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  20, 1200,    0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: We find item 1200 has no connection with user 6\n",
    "j = 1200\n",
    "neg_triplet[1] = j\n",
    "neg_triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebd06c",
   "metadata": {},
   "source": [
    "##### *Define metrics:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1881db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def getHitRatio(recommend_list, gt_item):\n",
    "    if gt_item in recommend_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def getNDCG(recommend_list, gt_item):\n",
    "    idx = np.where(recommend_list == gt_item)[0]\n",
    "    if len(idx) > 0:\n",
    "        return math.log(2)/math.log(idx+2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9094d9",
   "metadata": {},
   "source": [
    "##### *Build test dataset for evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3167c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paula\n",
    "dataset_path = 'data/movielens'\n",
    "#test_data = pd.read_csv(f'{dataset_path}.test.rating', sep='\\t',\n",
    "#                        header=None, names=colnames).to_numpy()\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b07ceeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       1,        1,        1, 20200519],\n",
       "       [       2,       27,        1, 20200613],\n",
       "       [       3,       56,        1, 20200528],\n",
       "       ...,\n",
       "       [    9998,    15939,        1, 20200901],\n",
       "       [    9999,      676,        1, 20200908],\n",
       "       [   10000,    12013,        1, 20200920]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=numpy.array(test_data_list)\n",
    "test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f2f27db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "48349\n"
     ]
    }
   ],
   "source": [
    "# Take number of users and items from reindex items from train set\n",
    "users, items = np.max(reindex_items, axis=0)[:2] + 1 # [ 943, 1682])\n",
    "print(users)\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdb96b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105365/3581757753.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pairs_test = test_data[:, :2].astype(np.int) - 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0, 10000],\n",
       "       [    1, 10026],\n",
       "       [    2, 10055],\n",
       "       ...,\n",
       "       [ 9997, 25938],\n",
       "       [ 9998, 10675],\n",
       "       [ 9999, 22012]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reindex test items and substract 1\n",
    "pairs_test = test_data[:, :2].astype(np.int) - 1    \n",
    "pairs_test[:, 1] = pairs_test[:, 1] + users \n",
    "pairs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "605d3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert 74 + 943 - 1 == 1016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a9abed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 10000])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = pairs_test[0]\n",
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5a8ccad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16281, 35398, 32243, 22542, 22530, 30720, 30896, 32929, 23340, 31486]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GENERATE TEST SET WITH NEGATIVE EXAMPLES TO EVALUATE\n",
    "max_users, max_items = field_dims[:2] # number users (943), number items (2625)\n",
    "negatives = []\n",
    "for t in range(10):\n",
    "    j = np.random.randint(max_users, max_items)\n",
    "    while (pair[0], j) in train_mat or j == pair[1]:\n",
    "        j = np.random.randint(max_users, max_items)\n",
    "    negatives.append(j)\n",
    "negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49a18a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 10000],\n",
       "       [    0, 10000],\n",
       "       [    0, 10000],\n",
       "       [    0, 10000],\n",
       "       [    0, 10000],\n",
       "       [    0, 10000],\n",
       "       [    0, 10000],\n",
       "       [    0, 10000],\n",
       "       [    0, 10000],\n",
       "       [    0, 10000],\n",
       "       [    0, 10000]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_user_test_set = np.vstack([pair, ] * (len(negatives)+1))\n",
    "single_user_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a53137a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 10000],\n",
       "       [    0, 16281],\n",
       "       [    0, 35398],\n",
       "       [    0, 32243],\n",
       "       [    0, 22542],\n",
       "       [    0, 22530],\n",
       "       [    0, 30720],\n",
       "       [    0, 30896],\n",
       "       [    0, 32929],\n",
       "       [    0, 23340],\n",
       "       [    0, 31486]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_user_test_set[:, 1][1:] = negatives\n",
    "single_user_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5d673",
   "metadata": {},
   "source": [
    "#### **2. Building dataset and preparing data for the model ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "291254a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "class MovieLens100kDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    MovieLens 100k Dataset\n",
    "\n",
    "    Data preparation\n",
    "        treat samples with a rating less than 3 as negative samples\n",
    "\n",
    "    :param dataset_path: MovieLens dataset path\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, num_negatives_train=4, num_negatives_test=100, sep='\\t'):\n",
    "\n",
    "        colnames = [\"customer_id\", 'article_id', 'label', 't_dat']\n",
    "        data = pd.read_csv(f'{dataset_path}.train.rating', sep=sep, header=None, names=colnames).to_numpy()\n",
    "        test_data = pd.read_csv(f'{dataset_path}.test.rating', sep=sep, header=None, names=colnames).to_numpy()\n",
    "\n",
    "        # TAKE items, targets and test_items\n",
    "        self.targets = data[:, 2]\n",
    "        self.items = self.preprocess_items(data)\n",
    "\n",
    "        # Save dimensions of max users and items and build training matrix\n",
    "        self.field_dims = np.max(self.items, axis=0) + 1 # ([ 943, 2625])\n",
    "        self.train_mat = build_adj_mx(self.field_dims[-1], self.items.copy())\n",
    "\n",
    "        # Generate train interactions with 4 negative samples for each positive\n",
    "        self.negative_sampling(num_negatives=num_negatives_train)\n",
    "        \n",
    "        # Build test set by passing as input the test item interactions\n",
    "        self.test_set = self.build_test_set(self.preprocess_items(test_data),\n",
    "                                            num_neg_samples_test = num_negatives_test)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.targets.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.interactions[index]\n",
    "    \n",
    "    def preprocess_items(self, data, users=number_cust_file): # users=25000):\n",
    "        reindexed_items = data[:, :2].astype(np.int) - 1  # -1 because ID begins from 1\n",
    "        #users, items = np.max(reindexed_items, axis=0)[:2] + 1 # [ 943, 1682])\n",
    "        # Reindex items (we need to have [users + items] nodes with unique idx)\n",
    "        reindexed_items[:, 1] = reindexed_items[:, 1] + users\n",
    "\n",
    "        return reindexed_items\n",
    "\n",
    "    def negative_sampling(self, num_negatives=4):\n",
    "        self.interactions = []\n",
    "        data = np.c_[(self.items, self.targets)].astype(int)\n",
    "        max_users, max_items = self.field_dims[:2] # number users (943), number items (2625)\n",
    "\n",
    "        for x in tqdm(data, desc=\"Performing negative sampling on test data...\"):  # x are triplets (u, i , 1) \n",
    "            # Append positive interaction\n",
    "            self.interactions.append(x)\n",
    "            # Copy user and maintain last position to 0. Now we will need to update neg_triplet[1] with j\n",
    "            neg_triplet = np.vstack([x, ] * (num_negatives))\n",
    "            neg_triplet[:, 2] = np.zeros(num_negatives)\n",
    "\n",
    "            # Generate num_negatives negative interactions\n",
    "            for idx in range(num_negatives):\n",
    "                j = np.random.randint(max_users, max_items)\n",
    "                # IDEA: Loop to exclude true interactions (set to 1 in adj_train) user - item\n",
    "                while (x[0], j) in self.train_mat:\n",
    "                    j = np.random.randint(max_users, max_items)\n",
    "                neg_triplet[:, 1][idx] = j\n",
    "            self.interactions.append(neg_triplet.copy())\n",
    "\n",
    "        self.interactions = np.vstack(self.interactions)\n",
    "    \n",
    "    def build_test_set(self, gt_test_interactions, num_neg_samples_test=99):\n",
    "        max_users, max_items = self.field_dims[:2] # number users (943), number items (2625)\n",
    "        test_set = []\n",
    "        for pair in tqdm(gt_test_interactions, desc=\"BUILDING TEST SET...\"):\n",
    "            negatives = []\n",
    "            for t in range(num_neg_samples_test):\n",
    "                j = np.random.randint(max_users, max_items)\n",
    "                while (pair[0], j) in self.train_mat or j == pair[1]:\n",
    "                    j = np.random.randint(max_users, max_items)\n",
    "                negatives.append(j)\n",
    "            #APPEND TEST SETS FOR SINGLE USER\n",
    "            single_user_test_set = np.vstack([pair, ] * (len(negatives)+1))\n",
    "            single_user_test_set[:, 1][1:] = negatives\n",
    "            test_set.append(single_user_test_set.copy())\n",
    "        return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39ba5384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/movielens\n"
     ]
    }
   ],
   "source": [
    "print (dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1275d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105365/3589377597.py:46: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  reindexed_items = data[:, :2].astype(np.int) - 1  # -1 because ID begins from 1\n",
      "BUILDING ADJACENCY MATRIX...: 100%|█████████████████████████████████████████████████████████████████| 397717/397717 [00:05<00:00, 73981.20it/s]\n",
      "Performing negative sampling on test data...: 100%|█████████████████████████████████████████████████| 397717/397717 [00:08<00:00, 44403.79it/s]\n",
      "BUILDING TEST SET...: 100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:03<00:00, 3198.27it/s]\n"
     ]
    }
   ],
   "source": [
    "full_dataset= MovieLens100kDataset( \\\n",
    "                                   dataset_path, num_negatives_train=4, num_negatives_test=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a2ca386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 10001,     1],\n",
       "       [    0, 26658,     0],\n",
       "       [    0, 20053,     0],\n",
       "       ...,\n",
       "       [ 9999, 17116,     0],\n",
       "       [ 9999, 32984,     0],\n",
       "       [ 9999, 18622,     0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90570 interactions with pairs of index that have interacted + 4*90570 negative\n",
    "full_dataset.interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98128480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 10001,     1],\n",
       "       [    0, 26658,     0],\n",
       "       [    0, 20053,     0],\n",
       "       [    0, 13869,     0],\n",
       "       [    0, 11827,     0],\n",
       "       [    0, 10002,     1],\n",
       "       [    0, 35862,     0],\n",
       "       [    0, 41424,     0],\n",
       "       [    0, 15524,     0],\n",
       "       [    0, 43613,     0],\n",
       "       [    0, 10003,     1],\n",
       "       [    0, 31870,     0],\n",
       "       [    0, 28245,     0],\n",
       "       [    0, 15138,     0],\n",
       "       [    0, 30822,     0],\n",
       "       [    0, 10004,     1],\n",
       "       [    0, 14985,     0],\n",
       "       [    0, 19500,     0],\n",
       "       [    0, 37273,     0],\n",
       "       [    0, 21466,     0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.interactions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d51a6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We had 99057 interactions in training_matrix --> now we have 99057 positive plus 4*99057 negative\n",
    "# assert 5*99057 == full_dataset.interactions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "caaff6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# For test set, we keep the size (one interaction per user) but we append 99 negative samples for evaluation\n",
    "print(len(full_dataset.test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "419e2ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dataset.test_set[0]) # --> [gt_pair + 99_neg_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ed4e59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 10000],\n",
       "       [    0, 21389],\n",
       "       [    0, 36712],\n",
       "       [    0, 20489],\n",
       "       [    0, 31409],\n",
       "       [    0, 28020],\n",
       "       [    0, 32606],\n",
       "       [    0, 27487],\n",
       "       [    0, 31931],\n",
       "       [    0, 44568],\n",
       "       [    0, 28857],\n",
       "       [    0, 39062],\n",
       "       [    0, 35802],\n",
       "       [    0, 30037],\n",
       "       [    0, 23601],\n",
       "       [    0, 13516],\n",
       "       [    0, 41483],\n",
       "       [    0, 48254],\n",
       "       [    0, 43162],\n",
       "       [    0, 25568],\n",
       "       [    0, 38834],\n",
       "       [    0, 10373],\n",
       "       [    0, 38832],\n",
       "       [    0, 15171],\n",
       "       [    0, 45629],\n",
       "       [    0, 42978],\n",
       "       [    0, 18580],\n",
       "       [    0, 47518],\n",
       "       [    0, 30466],\n",
       "       [    0, 36122],\n",
       "       [    0, 43177],\n",
       "       [    0, 47095],\n",
       "       [    0, 24160],\n",
       "       [    0, 32216],\n",
       "       [    0, 14517],\n",
       "       [    0, 10288],\n",
       "       [    0, 42680],\n",
       "       [    0, 33262],\n",
       "       [    0, 22557],\n",
       "       [    0, 42851],\n",
       "       [    0, 35897],\n",
       "       [    0, 11681],\n",
       "       [    0, 35086],\n",
       "       [    0, 32421],\n",
       "       [    0, 35010],\n",
       "       [    0, 19404],\n",
       "       [    0, 43530],\n",
       "       [    0, 23945],\n",
       "       [    0, 45421],\n",
       "       [    0, 37297],\n",
       "       [    0, 28497],\n",
       "       [    0, 32066],\n",
       "       [    0, 20871],\n",
       "       [    0, 28059],\n",
       "       [    0, 35474],\n",
       "       [    0, 10100],\n",
       "       [    0, 13392],\n",
       "       [    0, 47507],\n",
       "       [    0, 19860],\n",
       "       [    0, 29503],\n",
       "       [    0, 33670],\n",
       "       [    0, 48137],\n",
       "       [    0, 27287],\n",
       "       [    0, 12612],\n",
       "       [    0, 20854],\n",
       "       [    0, 44840],\n",
       "       [    0, 38287],\n",
       "       [    0, 39831],\n",
       "       [    0, 19784],\n",
       "       [    0, 13762],\n",
       "       [    0, 37132],\n",
       "       [    0, 38678],\n",
       "       [    0, 33510],\n",
       "       [    0, 38098],\n",
       "       [    0, 12456],\n",
       "       [    0, 24693],\n",
       "       [    0, 17163],\n",
       "       [    0, 23341],\n",
       "       [    0, 46069],\n",
       "       [    0, 34295],\n",
       "       [    0, 26151],\n",
       "       [    0, 47524],\n",
       "       [    0, 40726],\n",
       "       [    0, 13126],\n",
       "       [    0, 20361],\n",
       "       [    0, 34956],\n",
       "       [    0, 31274],\n",
       "       [    0, 40908],\n",
       "       [    0, 37936],\n",
       "       [    0, 43823],\n",
       "       [    0, 42971],\n",
       "       [    0, 30339],\n",
       "       [    0, 23016],\n",
       "       [    0, 17931],\n",
       "       [    0, 11852],\n",
       "       [    0, 14952],\n",
       "       [    0, 46308],\n",
       "       [    0, 36459],\n",
       "       [    0, 14769],\n",
       "       [    0, 12408]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc39419",
   "metadata": {},
   "source": [
    "Sampling 4 negative samples for each positive, will also work as a type of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dec37199",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(full_dataset, batch_size=256, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5878406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, (interactions) in enumerate(data_loader):\n",
    "    if i == 0:\n",
    "        print(interactions.shape)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf14787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b363ac0",
   "metadata": {},
   "source": [
    "### **Building Factorization Machines model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c835424",
   "metadata": {},
   "source": [
    "\n",
    "Our training matrix is now even sparser: Of all 237,746,250 values (90,570*2,625), only 181,140 are non-zero (90,570*2). In other words, the matrix is 99.92% sparse. Storing this as a dense matrix would be a massive waste of both storage and computing power!\n",
    "To avoid this, let’s use a scipy.lil_matrix sparse matrix for samples and a numpy array for labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b30fa1",
   "metadata": {},
   "source": [
    "<div>\n",
    "<center><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/04/03/sagemaker-factorization-1.gif\" width=\"400\"/></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19523718",
   "metadata": {},
   "source": [
    "##### **LAYERS:** Linear and FM part of the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e9e0b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING PYTORCH: https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b079f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear part of the equation\n",
    "class FeaturesLinear(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, output_dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = torch.nn.Embedding(field_dims, output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        # self.fc(x).shape --> [batch_size, num_fields, 1]\n",
    "        # torch.sum(self.fc(x), dim=1).shape --> ([batch_size, 1])\n",
    "        return torch.sum(self.fc(x), dim=1) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e294919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FM part of the equation\n",
    "class FM_operation(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9cd5a",
   "metadata": {},
   "source": [
    "##### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8799416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        # field_dims == total of nodes (sum users + context)\n",
    "        # self.linear = torch.nn.Linear(field_dims, 1, bias=True)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.embedding = torch.nn.Embedding(field_dims, embed_dim, sparse=False)\n",
    "        self.fm = FM_operation(reduce_sum=True)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, interaction_pairs):\n",
    "        \"\"\"\n",
    "        :param interaction_pairs: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        out = self.linear(interaction_pairs) + self.fm(self.embedding(interaction_pairs))\n",
    "        \n",
    "        return out.squeeze(1)\n",
    "        \n",
    "    def predict(self, interactions, device):\n",
    "        # return the score, inputs are numpy arrays, outputs are tensors\n",
    "        test_interactions = torch.from_numpy(interactions).to(dtype=torch.long, device=device)\n",
    "        output_scores = self.forward(test_interactions)\n",
    "        return output_scores\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e61850",
   "metadata": {},
   "source": [
    "### **Workflow for FM with usual embeddings ...**Ç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a5758",
   "metadata": {},
   "source": [
    "#### **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6a148b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, criterion, device, log_interval=100):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "\n",
    "    for i, (interactions) in enumerate(data_loader):\n",
    "        interactions = interactions.to(device)\n",
    "        targets = interactions[:,2]\n",
    "        predictions = model(interactions[:,:2])\n",
    "        \n",
    "        loss = criterion(predictions, targets.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    return mean(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d4789",
   "metadata": {},
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62daaad",
   "metadata": {},
   "source": [
    "##### **Understanding evaluation ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "add343b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_dataset.test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79d69617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_test = full_dataset.test_set[0]\n",
    "user_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2536af33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 10000],\n",
       "       [    0, 21389],\n",
       "       [    0, 36712],\n",
       "       [    0, 20489],\n",
       "       [    0, 31409],\n",
       "       [    0, 28020],\n",
       "       [    0, 32606],\n",
       "       [    0, 27487],\n",
       "       [    0, 31931],\n",
       "       [    0, 44568],\n",
       "       [    0, 28857],\n",
       "       [    0, 39062],\n",
       "       [    0, 35802],\n",
       "       [    0, 30037],\n",
       "       [    0, 23601],\n",
       "       [    0, 13516],\n",
       "       [    0, 41483],\n",
       "       [    0, 48254],\n",
       "       [    0, 43162],\n",
       "       [    0, 25568],\n",
       "       [    0, 38834],\n",
       "       [    0, 10373],\n",
       "       [    0, 38832],\n",
       "       [    0, 15171],\n",
       "       [    0, 45629],\n",
       "       [    0, 42978],\n",
       "       [    0, 18580],\n",
       "       [    0, 47518],\n",
       "       [    0, 30466],\n",
       "       [    0, 36122],\n",
       "       [    0, 43177],\n",
       "       [    0, 47095],\n",
       "       [    0, 24160],\n",
       "       [    0, 32216],\n",
       "       [    0, 14517],\n",
       "       [    0, 10288],\n",
       "       [    0, 42680],\n",
       "       [    0, 33262],\n",
       "       [    0, 22557],\n",
       "       [    0, 42851],\n",
       "       [    0, 35897],\n",
       "       [    0, 11681],\n",
       "       [    0, 35086],\n",
       "       [    0, 32421],\n",
       "       [    0, 35010],\n",
       "       [    0, 19404],\n",
       "       [    0, 43530],\n",
       "       [    0, 23945],\n",
       "       [    0, 45421],\n",
       "       [    0, 37297],\n",
       "       [    0, 28497],\n",
       "       [    0, 32066],\n",
       "       [    0, 20871],\n",
       "       [    0, 28059],\n",
       "       [    0, 35474],\n",
       "       [    0, 10100],\n",
       "       [    0, 13392],\n",
       "       [    0, 47507],\n",
       "       [    0, 19860],\n",
       "       [    0, 29503],\n",
       "       [    0, 33670],\n",
       "       [    0, 48137],\n",
       "       [    0, 27287],\n",
       "       [    0, 12612],\n",
       "       [    0, 20854],\n",
       "       [    0, 44840],\n",
       "       [    0, 38287],\n",
       "       [    0, 39831],\n",
       "       [    0, 19784],\n",
       "       [    0, 13762],\n",
       "       [    0, 37132],\n",
       "       [    0, 38678],\n",
       "       [    0, 33510],\n",
       "       [    0, 38098],\n",
       "       [    0, 12456],\n",
       "       [    0, 24693],\n",
       "       [    0, 17163],\n",
       "       [    0, 23341],\n",
       "       [    0, 46069],\n",
       "       [    0, 34295],\n",
       "       [    0, 26151],\n",
       "       [    0, 47524],\n",
       "       [    0, 40726],\n",
       "       [    0, 13126],\n",
       "       [    0, 20361],\n",
       "       [    0, 34956],\n",
       "       [    0, 31274],\n",
       "       [    0, 40908],\n",
       "       [    0, 37936],\n",
       "       [    0, 43823],\n",
       "       [    0, 42971],\n",
       "       [    0, 30339],\n",
       "       [    0, 23016],\n",
       "       [    0, 17931],\n",
       "       [    0, 11852],\n",
       "       [    0, 14952],\n",
       "       [    0, 46308],\n",
       "       [    0, 36459],\n",
       "       [    0, 14769],\n",
       "       [    0, 12408]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "44f254c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_pair: [    0 10000]\n",
      "lenght neg_items: 99\n"
     ]
    }
   ],
   "source": [
    "gt_pair = user_test[0]\n",
    "neg_items = user_test[1:]\n",
    "print(f'gt_pair: {gt_pair}')\n",
    "print(f'lenght neg_items: {len(neg_items)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1c79902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEFINE GT_ITEM\n",
    "gt_item = user_test[0][1]\n",
    "gt_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77bf6f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining dummy model with 8 embedding dimensions\n",
    "dummy_model = FactorizationMachineModel(full_dataset.field_dims[-1], 8).to(device)\n",
    "out = dummy_model.predict(user_test, device)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc913b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55d715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44af726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c458ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del out\n",
    "#torch.cuda.empty_cache()\n",
    "#dump_tensors()\n",
    "#del dummy_model\n",
    "#import gc\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "84f4ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "    \"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "    assert(isinstance(size, torch.Size))\n",
    "    return \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "    \"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "    import gc\n",
    "    total_size = 0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "                                          \" GPU\" if obj.is_cuda else \"\",\n",
    "                                          \" pinned\" if obj.is_pinned else \"\",\n",
    "                                          pretty_size(obj.size())))\n",
    "                    total_size += obj.numel()\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "                                                   type(obj.data).__name__, \n",
    "                                                   \" GPU\" if obj.is_cuda else \"\",\n",
    "                                                   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "                                                   \" grad\" if obj.requires_grad else \"\", \n",
    "                                                   \" volatile\" if obj.volatile else \"\",\n",
    "                                                   pretty_size(obj.data.size())))\n",
    "                    total_size += obj.data.numel()\n",
    "        except Exception as e:\n",
    "            pass        \n",
    "    print(\"Total size:\", total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8864deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9008e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1754b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2eb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af385565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0735,  0.2419,  0.6175,  0.6738,  0.5040,  0.3190,  1.7319,  0.4291,\n",
       "        -0.3535,  1.6929], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first 10 predictions, where 1st one is the one for the GT\n",
    "out[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "020fd92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.0175, 2.7743, 2.6291, 2.5063, 2.3616, 2.3297, 2.2282, 2.2028, 1.8737,\n",
      "        1.8242], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "[86 41 78 57 31 36 89 66 85 87]\n"
     ]
    }
   ],
   "source": [
    "values, indices = torch.topk(out, 10)\n",
    "print(values)\n",
    "print(indices.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c0ba25b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 10000])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "83ea063c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31274, 11681, 46069, 47507, 47095, 42680, 43823, 38287, 34956,\n",
       "       40908])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RANKING LIST TO RECOMMEND\n",
    "recommend_list = user_test[indices.cpu().detach().numpy()][:, 1]\n",
    "recommend_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff5c4102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_item in recommend_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35da0d8",
   "metadata": {},
   "source": [
    "##### **Defining test function...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "339f10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, full_dataset, device, topk=10):\n",
    "    # Test the HR and NDCG for the model @topK\n",
    "    model.eval()\n",
    "\n",
    "    HR, NDCG = [], []\n",
    "\n",
    "    for user_test in full_dataset.test_set:\n",
    "        gt_item = user_test[0][1]\n",
    "\n",
    "        predictions = model.predict(user_test, device)\n",
    "        _, indices = torch.topk(predictions, topk)\n",
    "        recommend_list = user_test[indices.cpu().detach().numpy()][:, 1]\n",
    "\n",
    "        HR.append(getHitRatio(recommend_list, gt_item))\n",
    "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
    "    return mean(HR), mean(NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f53cdf",
   "metadata": {},
   "source": [
    "#### **Model, loss and optimizer definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e0339c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FactorizationMachineModel(full_dataset.field_dims[-1], 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "943cd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27e531",
   "metadata": {},
   "source": [
    "#### **Random evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "11010fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial HR:  0.102\n",
      "initial NDCG:  0.04740044522753258\n"
     ]
    }
   ],
   "source": [
    "topk = 10\n",
    "\n",
    "# Check Init performance\n",
    "hr, ndcg = test(model, full_dataset, device, topk=topk)\n",
    "print(\"initial HR: \", hr)\n",
    "print(\"initial NDCG: \", ndcg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff92d4",
   "metadata": {},
   "source": [
    "#### **Start training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e44092b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch 0:\n",
      "training loss = 0.7095 | Eval: HR@10 = 0.1113, NDCG@10 = 0.0520 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 1:\n",
      "training loss = 0.5176 | Eval: HR@10 = 0.1507, NDCG@10 = 0.0764 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 2:\n",
      "training loss = 0.3981 | Eval: HR@10 = 0.1646, NDCG@10 = 0.0848 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 3:\n",
      "training loss = 0.3322 | Eval: HR@10 = 0.1701, NDCG@10 = 0.0877 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 4:\n",
      "training loss = 0.2788 | Eval: HR@10 = 0.1737, NDCG@10 = 0.0895 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 5:\n",
      "training loss = 0.2292 | Eval: HR@10 = 0.1764, NDCG@10 = 0.0903 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 6:\n",
      "training loss = 0.1843 | Eval: HR@10 = 0.1781, NDCG@10 = 0.0912 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 7:\n",
      "training loss = 0.1455 | Eval: HR@10 = 0.1793, NDCG@10 = 0.0918 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 8:\n",
      "training loss = 0.1134 | Eval: HR@10 = 0.1828, NDCG@10 = 0.0932 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 9:\n",
      "training loss = 0.0874 | Eval: HR@10 = 0.1828, NDCG@10 = 0.0938 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 10:\n",
      "training loss = 0.0667 | Eval: HR@10 = 0.1843, NDCG@10 = 0.0941 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 11:\n",
      "training loss = 0.0505 | Eval: HR@10 = 0.1853, NDCG@10 = 0.0947 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 12:\n",
      "training loss = 0.0379 | Eval: HR@10 = 0.1867, NDCG@10 = 0.0952 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 13:\n",
      "training loss = 0.0283 | Eval: HR@10 = 0.1882, NDCG@10 = 0.0958 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 14:\n",
      "training loss = 0.0209 | Eval: HR@10 = 0.1892, NDCG@10 = 0.0962 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 15:\n",
      "training loss = 0.0154 | Eval: HR@10 = 0.1906, NDCG@10 = 0.0971 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 16:\n",
      "training loss = 0.0112 | Eval: HR@10 = 0.1915, NDCG@10 = 0.0978 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 17:\n",
      "training loss = 0.0081 | Eval: HR@10 = 0.1926, NDCG@10 = 0.0983 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 18:\n",
      "training loss = 0.0059 | Eval: HR@10 = 0.1939, NDCG@10 = 0.0991 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 19:\n",
      "training loss = 0.0042 | Eval: HR@10 = 0.1943, NDCG@10 = 0.0995 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO EPOCHS NOW\n",
    "tb = True\n",
    "topk = 10\n",
    "for epoch_i in range(20):\n",
    "    #data_loader.dataset.negative_sampling()\n",
    "    train_loss = train_one_epoch(model, optimizer, data_loader, criterion, device)\n",
    "    hr, ndcg = test(model, full_dataset, device, topk=topk)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print(f'epoch {epoch_i}:')\n",
    "    print(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} ')\n",
    "    print('\\n')\n",
    "    if tb:\n",
    "        tb_fm.add_scalar('train/loss', train_loss, epoch_i)\n",
    "        tb_fm.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
    "        tb_fm.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "52ad4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir runs2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab2610a",
   "metadata": {},
   "source": [
    "### **Computing embeddings with GCN instead ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc270561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     1,     2,  ..., 48346, 48347, 48348],\n",
       "                       [    0,     1,     2,  ..., 48346, 48347, 48348]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(48349, 48349), nnz=48349, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import identity\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\" Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "# BUILD FEATURES X (identity tensor for the moment)\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html?highlight=sparse#torch_geometric.utils.from_scipy_sparse_matrix\n",
    "X = sparse_mx_to_torch_sparse_tensor(identity(full_dataset.train_mat.shape[0]))\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0fdc08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<48349x48349 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 749344 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get TRAIN MX FROM DATASET\n",
    "full_dataset.train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "60228c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 10001,     0,  ..., 24434,  9999, 34118],\n",
       "        [10001,     0, 10002,  ...,  9999, 34118,  9999]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUILD EDGE INDEX from training matrix (identity tensor for the moment)\n",
    "edge_idx, edge_attr = from_scipy_sparse_matrix(full_dataset.train_mat)\n",
    "edge_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a15d774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv # https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html?highlight=GCNConv#torch_geometric.nn.conv.GCNConv\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "\n",
    "class GraphModel(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, features, train_mat, attention=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.A = train_mat\n",
    "        self.features = features\n",
    "        if attention:\n",
    "            self.GCN_module = GATConv(int(field_dims), embed_dim, heads=8, dropout=0.6)\n",
    "        else:  \n",
    "            self.GCN_module = GCNConv(field_dims, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        return self.GCN_module(self.features, self.A)[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea95daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel_withGCN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Factorization Machine.\n",
    "\n",
    "    Reference:\n",
    "        S Rendle, Factorization Machines, 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, X, A, attention=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        #self.embedding = torch.nn.Embedding(field_dims, embed_dim, sparse=False)\n",
    "        self.embedding = GraphModel(field_dims, embed_dim, X, A, attention=attention)\n",
    "        self.fm = FM_operation(reduce_sum=True)\n",
    "\n",
    "        #torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, interaction_pairs):\n",
    "        \"\"\"\n",
    "        :param interaction_pairs: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        out = self.linear(interaction_pairs) + self.fm(self.embedding(interaction_pairs))\n",
    "        return out.squeeze(1)\n",
    "        \n",
    "    def predict(self, interactions, device):\n",
    "        # return the score, inputs are numpy arrays, outputs are tensors\n",
    " \n",
    "        test_interactions = torch.from_numpy(interactions).to(dtype=torch.long, device=device)\n",
    "        output_scores = self.forward(test_interactions)\n",
    "        return output_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072dba2",
   "metadata": {},
   "source": [
    "#### **Using GCN regular layer ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92b43394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.device(\"cuda:1\")\n",
    "model_gcn = FactorizationMachineModel_withGCN(full_dataset.field_dims[-1],\n",
    "                                              64,\n",
    "                                              X.to(device),\n",
    "                                              edge_idx.to(device),\n",
    "                                              ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7701762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(params=model_gcn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4eb5df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial HR:  0.1042\n",
      "initial NDCG:  0.04732804796209067\n"
     ]
    }
   ],
   "source": [
    "topk = 10\n",
    "\n",
    "# Check Init performance (already higher than init performance with FM and usual embeddings)\n",
    "hr, ndcg = test(model_gcn, full_dataset, device, topk=topk)\n",
    "print(\"initial HR: \", hr)\n",
    "print(\"initial NDCG: \", ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6376386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch 0:\n",
      "training loss = 0.5893 | Eval: HR@10 = 0.2360, NDCG@10 = 0.1252 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 1:\n",
      "training loss = 0.4291 | Eval: HR@10 = 0.2827, NDCG@10 = 0.1521 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 2:\n",
      "training loss = 0.3856 | Eval: HR@10 = 0.2907, NDCG@10 = 0.1552 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 3:\n",
      "training loss = 0.3617 | Eval: HR@10 = 0.3014, NDCG@10 = 0.1596 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 4:\n",
      "training loss = 0.3417 | Eval: HR@10 = 0.3072, NDCG@10 = 0.1632 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 5:\n",
      "training loss = 0.3208 | Eval: HR@10 = 0.3130, NDCG@10 = 0.1671 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 6:\n",
      "training loss = 0.2978 | Eval: HR@10 = 0.3189, NDCG@10 = 0.1724 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 7:\n",
      "training loss = 0.2730 | Eval: HR@10 = 0.3229, NDCG@10 = 0.1753 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 8:\n",
      "training loss = 0.2473 | Eval: HR@10 = 0.3291, NDCG@10 = 0.1800 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 9:\n",
      "training loss = 0.2216 | Eval: HR@10 = 0.3357, NDCG@10 = 0.1856 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 10:\n",
      "training loss = 0.1967 | Eval: HR@10 = 0.3403, NDCG@10 = 0.1892 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 11:\n",
      "training loss = 0.1732 | Eval: HR@10 = 0.3464, NDCG@10 = 0.1940 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 12:\n",
      "training loss = 0.1515 | Eval: HR@10 = 0.3481, NDCG@10 = 0.1959 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 13:\n",
      "training loss = 0.1315 | Eval: HR@10 = 0.3546, NDCG@10 = 0.2006 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 14:\n",
      "training loss = 0.1134 | Eval: HR@10 = 0.3576, NDCG@10 = 0.2041 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 15:\n",
      "training loss = 0.0971 | Eval: HR@10 = 0.3620, NDCG@10 = 0.2070 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 16:\n",
      "training loss = 0.0827 | Eval: HR@10 = 0.3642, NDCG@10 = 0.2087 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 17:\n",
      "training loss = 0.0699 | Eval: HR@10 = 0.3663, NDCG@10 = 0.2109 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 18:\n",
      "training loss = 0.0587 | Eval: HR@10 = 0.3698, NDCG@10 = 0.2131 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 19:\n",
      "training loss = 0.0489 | Eval: HR@10 = 0.3694, NDCG@10 = 0.2136 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO EPOCHS NOW\n",
    "tb = True\n",
    "for epoch_i in range(20):\n",
    "    #data_loader.dataset.negative_sampling()\n",
    "    train_loss = train_one_epoch(model_gcn, optimizer, data_loader, criterion, device)\n",
    "    hr, ndcg = test(model_gcn, full_dataset, device, topk=topk)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print(f'epoch {epoch_i}:')\n",
    "    print(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} ')\n",
    "    print('\\n')\n",
    "    if tb:\n",
    "        tb_gcn.add_scalar('train/loss', train_loss, epoch_i)\n",
    "        tb_gcn.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
    "        tb_gcn.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245cbff6",
   "metadata": {},
   "source": [
    "#### **Using GCN ATTENTION layer ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "561b6432",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_gcn_att' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m criterion\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m optimizer\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model_gcn_att\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_gcn_att' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "del model_gcn\n",
    "del hr\n",
    "del ndcg\n",
    "del criterion\n",
    "del optimizer\n",
    "del model_gcn_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "23e88b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: GPU pinned 100 × 2\n",
      "Tensor: GPU pinned 100 × 2 × 8\n",
      "Tensor: GPU pinned 100 × 8\n",
      "Tensor: GPU pinned 100 × 1\n",
      "Tensor: GPU pinned 100\n",
      "Tensor: GPU pinned 10\n",
      "Tensor: GPU pinned 10\n",
      "Tensor: GPU pinned 10\n",
      "Tensor: GPU pinned 1\n",
      "Tensor: GPU pinned 48349 × 1\n",
      "Tensor: GPU pinned 48349 × 32\n",
      "Parameter: GPU pinned 1\n",
      "Parameter: GPU pinned 48349 × 1\n",
      "Parameter: GPU pinned 48349 × 32\n",
      "Parameter: GPU pinned 1\n",
      "Parameter: GPU pinned 48349 × 1\n",
      "Parameter: GPU pinned 48349 × 8\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 64 × 48349\n",
      "Parameter: GPU pinned 64\n",
      "Parameter: GPU pinned 64 × 48349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laika/anaconda3/envs/jupyter_geo/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 9817808\n",
      "Tensor: GPU pinned 100 × 2\n",
      "Tensor: GPU pinned 100 × 2 × 8\n",
      "Tensor: GPU pinned 100 × 8\n",
      "Tensor: GPU pinned 100 × 1\n",
      "Tensor: GPU pinned 100\n",
      "Tensor: GPU pinned 10\n",
      "Tensor: GPU pinned 10\n",
      "Tensor: GPU pinned 10\n",
      "Tensor: GPU pinned 1\n",
      "Tensor: GPU pinned 48349 × 1\n",
      "Tensor: GPU pinned 48349 × 32\n",
      "Parameter: GPU pinned 1\n",
      "Parameter: GPU pinned 48349 × 1\n",
      "Parameter: GPU pinned 48349 × 32\n",
      "Parameter: GPU pinned 1\n",
      "Parameter: GPU pinned 48349 × 1\n",
      "Parameter: GPU pinned 48349 × 8\n",
      "Tensor: GPU pinned 64\n",
      "Tensor: GPU pinned 64 × 48349\n",
      "Parameter: GPU pinned 64 × 48349\n",
      "Parameter: GPU pinned 64\n",
      "Total size: 9817808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laika/anaconda3/envs/jupyter_geo/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dump_tensors()\n",
    "#device2 = torch.device(\"cuda:1\")\n",
    "torch.cuda.empty_cache()\n",
    "dump_tensors()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cf0e57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gcn_att = FactorizationMachineModel_withGCN(full_dataset.field_dims[-1],\n",
    "                                                  64,\n",
    "                                                  X.to(device),\n",
    "                                                  edge_idx.to(device),\n",
    "                                                  attention=True\n",
    "                                                  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "52fe27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(params=model_gcn_att.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b7c5bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial HR:  0.0931\n",
      "initial NDCG:  0.04210229620104079\n"
     ]
    }
   ],
   "source": [
    "topk = 10\n",
    "\n",
    "# Check Init performance (already higher than init performance with FM and usual embeddings)\n",
    "hr, ndcg = test(model_gcn_att, full_dataset, device, topk=topk)\n",
    "print(\"initial HR: \", hr)\n",
    "print(\"initial NDCG: \", ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "69dce4c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "epoch 0:\n",
      "training loss = 0.5800 | Eval: HR@10 = 0.2712, NDCG@10 = 0.1351 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 1:\n",
      "training loss = 0.4235 | Eval: HR@10 = 0.3081, NDCG@10 = 0.1582 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 2:\n",
      "training loss = 0.3777 | Eval: HR@10 = 0.3208, NDCG@10 = 0.1678 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 3:\n",
      "training loss = 0.3420 | Eval: HR@10 = 0.3384, NDCG@10 = 0.1800 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 4:\n",
      "training loss = 0.3066 | Eval: HR@10 = 0.3449, NDCG@10 = 0.1840 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 5:\n",
      "training loss = 0.2710 | Eval: HR@10 = 0.3577, NDCG@10 = 0.1942 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 6:\n",
      "training loss = 0.2360 | Eval: HR@10 = 0.3634, NDCG@10 = 0.1997 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 7:\n",
      "training loss = 0.2035 | Eval: HR@10 = 0.3747, NDCG@10 = 0.2093 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 8:\n",
      "training loss = 0.1753 | Eval: HR@10 = 0.3818, NDCG@10 = 0.2147 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 9:\n",
      "training loss = 0.1494 | Eval: HR@10 = 0.3814, NDCG@10 = 0.2178 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 10:\n",
      "training loss = 0.1280 | Eval: HR@10 = 0.3846, NDCG@10 = 0.2207 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 11:\n",
      "training loss = 0.1097 | Eval: HR@10 = 0.3862, NDCG@10 = 0.2232 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 12:\n",
      "training loss = 0.0935 | Eval: HR@10 = 0.3962, NDCG@10 = 0.2313 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 13:\n",
      "training loss = 0.0792 | Eval: HR@10 = 0.3923, NDCG@10 = 0.2291 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 14:\n",
      "training loss = 0.0684 | Eval: HR@10 = 0.3989, NDCG@10 = 0.2326 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 15:\n",
      "training loss = 0.0589 | Eval: HR@10 = 0.3988, NDCG@10 = 0.2329 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 16:\n",
      "training loss = 0.0506 | Eval: HR@10 = 0.4007, NDCG@10 = 0.2348 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 17:\n",
      "training loss = 0.0436 | Eval: HR@10 = 0.3998, NDCG@10 = 0.2356 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 18:\n",
      "training loss = 0.0386 | Eval: HR@10 = 0.4055, NDCG@10 = 0.2399 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch 19:\n",
      "training loss = 0.0339 | Eval: HR@10 = 0.4007, NDCG@10 = 0.2381 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO EPOCHS NOW\n",
    "tb = True\n",
    "for epoch_i in range(20):\n",
    "    #data_loader.dataset.negative_sampling()\n",
    "    train_loss = train_one_epoch(model_gcn_att, optimizer, data_loader, criterion, device)\n",
    "    hr, ndcg = test(model_gcn_att, full_dataset, device, topk=topk)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print(f'epoch {epoch_i}:')\n",
    "    print(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} ')\n",
    "    print('\\n')\n",
    "    if tb:\n",
    "        tb_gcn_attention.add_scalar('train/loss', train_loss, epoch_i)\n",
    "        tb_gcn_attention.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
    "        tb_gcn_attention.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed6368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90d5c358",
   "metadata": {},
   "source": [
    "## **VISUALIZING RESULTS**\n",
    "\n",
    "Once we have trained both models (*fm with usual embbedding layers* vs *fm with embeddings from gcn*), we can observe both metrics and loss in the same graphic in order to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "31519124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorboard import notebook\n",
    "#notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d921c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c3abb4d93eed9c3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c3abb4d93eed9c3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580525e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96696971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8354f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46632405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daea0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477dd4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4becbc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transactions.to_csv(\"./data/tmp.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_geo",
   "language": "python",
   "name": "jupyter_geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
